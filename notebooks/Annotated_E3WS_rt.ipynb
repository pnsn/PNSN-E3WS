{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotated_E3WS_rt.ipynb\n",
    "**:auth:** Nathan T Stevens  \n",
    "**:email:** ntsteven (at) uw.edu  \n",
    "**:org:** Pacific Northwest Seismic Network    \n",
    "**:license:**   \n",
    "    Creative Commons Attribution 4.0 International (inherited from E3WS repository license)  \n",
    "    (CC4.0-BY License)  \n",
    "**:Purpose:**  \n",
    "This notebook provides an annotated version of the example E3WS_rt.py script provided by Lara et al. (2023)  \n",
    "\n",
    "**:Attribution:**  \n",
    "This code modifies the code base from **E3WS, Pablo Lara et al. 2023, ** [![DOI](https://zenodo.org/badge/637827897.svg)](https://zenodo.org/badge/latestdoi/637827897)\n",
    "\n",
    "And their paper:   \n",
    "\n",
    "**Pablo Lara, Quentin Bletery, Jean-Paul Ampuero, Adolfo Inza, Hernando Tavera. Earthquake Early Warning Starting From 3 s of Records on a Single Station With Machine Learning. Journal of Geophysical Research: Solid Earth.**\n",
    "\n",
    "**E3WS article: https://doi.org/10.1029/2023JB026575**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import depdendences and do a bit of path definition work to re-define directory structure mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import collections\n",
    "import datetime\n",
    "import collections\n",
    "import numpy as np\n",
    "from obspy import read, UTCDateTime, Stream\n",
    "\n",
    "# Local E3WS Path Mapping\n",
    "E3WS_ROOT = os.path.join(\"..\", \"E3WS\")\n",
    "RT_ROOT = os.path.join(E3WS_ROOT, \"real_time\")\n",
    "sys.path.append(RT_ROOT)\n",
    "sys.path.append(os.path.join(RT_ROOT, \"functions\"))\n",
    "# E3WS Modules\n",
    "import pb_functions as pbi\n",
    "import pb_utils_v16 as pb_utils\n",
    "import pick_func_v16 as pick_func\n",
    "from utils import pb_SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a range of environmental parameters (need to decode these a bit...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software parameters\n",
    "flag_plot = 1  # plot detection, P-phase picking and first magnitude estimation\n",
    "flag_writefile = 1  # write logs\n",
    "flag_trackmag = True\n",
    "pb_version = 16\n",
    "pb_subversion = 0\n",
    "n_models = 5  # number of models for tracking source characterization, 1: just 3 sec model, 58: 3 sec to 60 sec models\n",
    "pb_inst = True  # True for instrument correction, you have to add pz/ folder with pzfile\n",
    "\n",
    "# Model parameters\n",
    "Ptrain = 0.5  # Dont change\n",
    "mov_time = 0.1  # moving time for estimate P-phase arrival time\n",
    "thr = 0.80  # detection threshold to trigger an event\n",
    "# For BAZ models\n",
    "n_models_baz = 1  # Predict just using 3 s of P-wave\n",
    "\n",
    "\n",
    "# Some more path defining\n",
    "# folder_models = \"../models/\"\n",
    "folder_models = os.path.join(E3WS_ROOT, \"models\")\n",
    "# folder_out = 'results'\n",
    "folder_out = os.path.join(RT_ROOT, \"results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load station metadata and Poles and Zeros (instrument response)  \n",
    "*TODO: Add option for attached responses in this workflow*  \n",
    "  \n",
    "AND  \n",
    "  \n",
    "Set up results logging file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pole Zero\n",
      "PZFILE: ../E3WS/real_time/pz/SLRZ/_HN.pz\n",
      "Folder out already exist\n"
     ]
    }
   ],
   "source": [
    "# Station parameters\n",
    "station = \"SLRZ\"\n",
    "sta_lat = -12.074200\n",
    "sta_lon = -77.233200\n",
    "\n",
    "if pb_inst == True:\n",
    "    print(\"Using Pole Zero\")\n",
    "    # pzfile = \"pz/\" + station + \"_HN.pz\"\n",
    "    pzfile = os.path.join(RT_ROOT,'pz',station,'_HN.pz')\n",
    "else:\n",
    "    print(\"NOT using Pole Zero\")\n",
    "    pzfile = None\n",
    "\n",
    "print(\"PZFILE:\", pzfile)\n",
    "\n",
    "#Earthquake name\n",
    "eq_name = 'Canta_M5.6_20220107'\n",
    "\n",
    "# Try to create output folder if model logging is on\n",
    "if flag_writefile == 1:\n",
    "    try:\n",
    "        os.makedirs(folder_out)\n",
    "    except Exception:\n",
    "        print(\"Folder out already exist\")\n",
    "\n",
    "    # Write headers to log file\n",
    "    f = open(os.path.join(folder_out,f'{station:s}_{eq_name:s}.csv'), \"w\")\n",
    "    f.write(\n",
    "        \"P_AI_date,starttime,endtime,P-wave(s),mag_pred(M),lat_pred,lon_pred,dis_pred(km),dep_pred(km),baz_pred(Â°)\\n\"\n",
    "    )\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Trace(s) in Stream:\n",
      "PE.SLRZ.14.HNE | 2022-01-07T10:27:01.300000Z - 2022-01-07T10:29:06.300000Z | 100.0 Hz, 12501 samples\n",
      "PE.SLRZ.14.HNN | 2022-01-07T10:27:01.300000Z - 2022-01-07T10:29:06.300000Z | 100.0 Hz, 12501 samples\n",
      "PE.SLRZ.14.HNZ | 2022-01-07T10:27:01.300000Z - 2022-01-07T10:29:06.300000Z | 100.0 Hz, 12501 samples\n"
     ]
    }
   ],
   "source": [
    "# Waveform file\n",
    "# org_time = obspy.UTCDateTime('2022-01-07T10:27:06.30')#By IGP, just for trim\n",
    "file_E = os.path.join(RT_ROOT, \"data\", eq_name, f\"{station}.HNE.PE.2022.007.mseed\")\n",
    "file_N = os.path.join(RT_ROOT, \"data\", eq_name, f\"{station}.HNN.PE.2022.007.mseed\")\n",
    "file_Z = os.path.join(RT_ROOT, \"data\", eq_name, f\"{station}.HNZ.PE.2022.007.mseed\")\n",
    "format = \"MSEED\"\n",
    "\n",
    "# Read waveforms\n",
    "st_raw = Stream()\n",
    "for _f in [file_E, file_N, file_Z]:\n",
    "    st_raw += read(_f)\n",
    "# data = \"data/\" + eq_name + \"/\"\n",
    "\n",
    "# st_raw = read(data + file_E, format=format)\n",
    "# st_raw += read(data + file_N, format=format)\n",
    "# st_raw += read(data + file_Z, format=format)\n",
    "\n",
    "# Print raw data\n",
    "print(st_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading models\n",
      "['MAG_Whole_StackingXGB_7tp3f45_v16.0.joblib', 'MAG_Whole_StackingXGB_7tp4f45_v16.0.joblib', 'MAG_Whole_StackingXGB_7tp5f45_v16.0.joblib', 'MAG_Whole_StackingXGB_7tp6f45_v16.0.joblib', 'MAG_Whole_StackingXGB_7tp7f45_v16.0.joblib']\n",
      "Reading models complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading models\")\n",
    "if flag_trackmag == 1:\n",
    "    # n_models = 1 #58 in total, 1 model, 1 estimation\n",
    "    MAG_md = [\n",
    "        \"MAG_Whole_StackingXGB_7tp\"\n",
    "        + str(i)\n",
    "        + \"f45_v\"\n",
    "        + str(pb_version)\n",
    "        + \".\"\n",
    "        + str(pb_subversion)\n",
    "        + \".joblib\"\n",
    "        for i in np.arange(n_models) + 3\n",
    "    ]\n",
    "    DIS_md = [\n",
    "        \"DIS_Whole_StackingXGB_7tp\"\n",
    "        + str(i)\n",
    "        + \"f45_v\"\n",
    "        + str(pb_version)\n",
    "        + \".\"\n",
    "        + str(pb_subversion)\n",
    "        + \".joblib\"\n",
    "        for i in np.arange(n_models) + 3\n",
    "    ]\n",
    "    DEP_md = [\n",
    "        \"DEP_Whole_StackingXGB_7tp\"\n",
    "        + str(i)\n",
    "        + \"f45_v\"\n",
    "        + str(pb_version)\n",
    "        + \".\"\n",
    "        + str(pb_subversion)\n",
    "        + \".joblib\"\n",
    "        for i in np.arange(n_models) + 3\n",
    "    ]\n",
    "    BAZ_cos_md = [\n",
    "        \"welloriented_BAZ_Cos_STEAD_StackingXGB_7tp\"\n",
    "        + str(i)\n",
    "        + \"f45_v\"\n",
    "        + str(pb_version)\n",
    "        + \".\"\n",
    "        + str(pb_subversion)\n",
    "        + \".joblib\"\n",
    "        for i in np.arange(n_models_baz) + 3\n",
    "    ]\n",
    "    BAZ_sin_md = [\n",
    "        \"welloriented_BAZ_Sin_STEAD_StackingXGB_7tp\"\n",
    "        + str(i)\n",
    "        + \"f45_v\"\n",
    "        + str(pb_version)\n",
    "        + \".\"\n",
    "        + str(pb_subversion)\n",
    "        + \".joblib\"\n",
    "        for i in np.arange(n_models_baz) + 3\n",
    "    ]\n",
    "\n",
    "    # print files:\n",
    "    print(MAG_md)\n",
    "\n",
    "    MAG_pb = [\n",
    "        joblib.load(open(os.path.join(E3WS_ROOT,'models',\"MAG\",MAG_md[i]), \"rb\"))\n",
    "        for i in range(n_models)\n",
    "    ]\n",
    "    DIS_pb = [\n",
    "        joblib.load(open(os.path.join(E3WS_ROOT,'models',\"DIS\",DIS_md[i]), \"rb\"))\n",
    "        for i in range(n_models)\n",
    "    ]\n",
    "    DEP_pb = [\n",
    "        joblib.load(open(os.path.join(E3WS_ROOT,'models',\"DEP\",DEP_md[i]), \"rb\"))\n",
    "        for i in range(n_models)\n",
    "    ]\n",
    "    BAZ_cos_pb = [\n",
    "        joblib.load(open(os.path.join(E3WS_ROOT,'models',\"BAZ\",BAZ_cos_md[i]), \"rb\"))\n",
    "        for i in range(n_models_baz)\n",
    "    ]\n",
    "    BAZ_sin_pb = [\n",
    "        joblib.load(open(os.path.join(E3WS_ROOT,'models',\"BAZ\",BAZ_sin_md[i]), \"rb\"))\n",
    "        for i in range(n_models_baz)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Reading DET and PICK model\n",
    "DET_pb = joblib.load(\n",
    "    open(\n",
    "        os.path.join(E3WS_ROOT,'models','DET',\"DET_\"\n",
    "        + station\n",
    "        + \"_Whole_XGB_10tp0.5to4.0each0.5f7_unbalanced_official_v16.joblib\"),\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "PICK_pb = joblib.load(\n",
    "    open(\n",
    "        os.path.join(E3WS_ROOT,'models','PICK','PICK_CLF_XGBdep4n6000bytree0.46level1.0node1.0_P0.5_NPS_eachpoint_spike_v16.joblib'),\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Reading models complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<utils.pb_SAM object at 0x2a8215ee0>, <utils.pb_SAM object at 0x2a825ad00>, <utils.pb_SAM object at 0x2ac4aefd0>, <utils.pb_SAM object at 0x2af3b8eb0>, <utils.pb_SAM object at 0x2ac4affa0>]\n",
      "<utils.pb_SAM object at 0x2a8215ee0>\n",
      "dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'xgbregressor', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'xgbregressor__objective', 'xgbregressor__base_score', 'xgbregressor__booster', 'xgbregressor__callbacks', 'xgbregressor__colsample_bylevel', 'xgbregressor__colsample_bynode', 'xgbregressor__colsample_bytree', 'xgbregressor__early_stopping_rounds', 'xgbregressor__enable_categorical', 'xgbregressor__eval_metric', 'xgbregressor__gamma', 'xgbregressor__gpu_id', 'xgbregressor__grow_policy', 'xgbregressor__importance_type', 'xgbregressor__interaction_constraints', 'xgbregressor__learning_rate', 'xgbregressor__max_bin', 'xgbregressor__max_cat_to_onehot', 'xgbregressor__max_delta_step', 'xgbregressor__max_depth', 'xgbregressor__max_leaves', 'xgbregressor__min_child_weight', 'xgbregressor__missing', 'xgbregressor__monotone_constraints', 'xgbregressor__n_estimators', 'xgbregressor__n_jobs', 'xgbregressor__num_parallel_tree', 'xgbregressor__predictor', 'xgbregressor__random_state', 'xgbregressor__reg_alpha', 'xgbregressor__reg_lambda', 'xgbregressor__sampling_method', 'xgbregressor__scale_pos_weight', 'xgbregressor__subsample', 'xgbregressor__tree_method', 'xgbregressor__validate_parameters', 'xgbregressor__verbosity', 'xgbregressor__nthread'])\n"
     ]
    }
   ],
   "source": [
    "# Look at properties of some of these model objects\n",
    "print(MAG_pb)\n",
    "_tmp = MAG_pb[0]\n",
    "print(_tmp)\n",
    "_tmp = _tmp.base_models_[0]\n",
    "\n",
    "_tmp = _tmp[0]\n",
    "print(_tmp.get_params().keys())\n",
    "# _tmp = _tmp[0].best_estimator_\n",
    "# # print(_tmp)\n",
    "# _tmp = _tmp[1]\n",
    "# print(_tmp)\n",
    "# print(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select ENZ\n",
    "st_e = st_raw.select(component=\"E\")\n",
    "st_n = st_raw.select(component=\"N\")\n",
    "st_z = st_raw.select(component=\"Z\")\n",
    "\n",
    "# Quality control of traces\n",
    "n_traces = min(len(st_e), len(st_n), len(st_z))\n",
    "\n",
    "display(n_traces)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate across number of traces in stream - this is quite a fragile way of handling multiple traces...\n",
    "for i in range(0, n_traces):\n",
    "    print(i, n_traces)\n",
    "    # Order components as ENZ (reverse of most picker routines, but typical alphabetical order of others)\n",
    "    st = Stream()\n",
    "    st = st.append(st_e[i])\n",
    "    st = st.append(st_n[i])\n",
    "    st = st.append(st_z[i])\n",
    "\n",
    "    # Do maximum complete length trimming\n",
    "    maxstart = np.max([tr.stats.starttime for tr in st])\n",
    "    minend = np.min([tr.stats.endtime for tr in st])\n",
    "    st.trim(maxstart, minend)\n",
    "\n",
    "    # Create copy of initial data (is this strictly necessary?)\n",
    "    st_process = st.copy()\n",
    "\n",
    "    # Get sampling rate\n",
    "    Fs = st_process[0].stats.sampling_rate\n",
    "    # TODO: Shift this to processing parameters - unnecessary\n",
    "    slidding_seconds = 1\n",
    "    # Get number of data in trace\n",
    "    N = len(st_process[0].data)\n",
    "    # Get step size\n",
    "    w_sec = 10\n",
    "    # Calculate number of windows\n",
    "    n_waves = int((N / Fs - w_sec) / slidding_seconds) + 1  # slidding 1s\n",
    "\n",
    "    # Define holder arrays - TODO: this can be improved by preallocating space using the metrics calculated above\n",
    "    PROB_P = np.array([])\n",
    "    PROB_N = np.array([])\n",
    "    PROB_S = np.array([])\n",
    "    PROB_PP = np.array([])\n",
    "    \n",
    "    # TODO: Remind myself of what the `collections.deque` method does...\n",
    "    buffer_proba = collections.deque(maxlen=5)\n",
    "    buffer_proba_n = collections.deque(maxlen=5)\n",
    "\n",
    "    # Set up counters?\n",
    "    eq_thr_past = 0\n",
    "    eq_thr_present = 0\n",
    "\n",
    "    flag_magnitude = 0\n",
    "    c_mag = 0  # counter magnitude (1 times)\n",
    "    waiting_sec = 0\n",
    "    n_eq = 0\n",
    "\n",
    "    # Define another holder array... probably shift this up to where the other holders are instantiated?\n",
    "    DL = np.array([])\n",
    "\n",
    "    # Iterate across windows - TODO: this can be DRASITCALLY simplified with a st.slide()?\n",
    "    for i3 in range(0, n_waves):\n",
    "        # Create ANOTHER COPY?\n",
    "        st_pb = st_process.copy()\n",
    "        # Get window starttime\n",
    "        t1 = i3 * slidding_seconds\n",
    "        # Get window endtime\n",
    "        t2 = w_sec + t1\n",
    "        # Trim window\n",
    "        st_pb = st_pb.trim(st_pb[0].stats.starttime + t1, st_pb[0].stats.starttime + t2)\n",
    "\n",
    "        ### CALCULATE FEATURE VECTOR INCLUDING PRE-PROCESSING\n",
    "        # A lot of this pre-processing can be done before windowing data to save repeat calculations\n",
    "        # Feature vector\n",
    "        FV = pb_utils.st_FV(st_pb, pb_inst, pzfile=pzfile, fmin=1.0, fmax=7.0)\n",
    "        FV = np.real([FV])\n",
    "\n",
    "        ### RUN DETECTION MODEL\n",
    "        # Get probabilities\n",
    "        prob_nps = DET_pb.predict_proba(FV)[0]\n",
    "        # Extract modeled values\n",
    "        prob_n = prob_nps[0]  # v8 - noise probability\n",
    "        prob_p = prob_nps[1]  # v8 - p-wave probability\n",
    "        prob_s = prob_nps[2]  # v8 - s-wave probability\n",
    "        # Output result (`black` code formatting)\n",
    "        print(\n",
    "            i3,\n",
    "            round(t2, 3),\n",
    "            st_pb[0].stats.starttime,\n",
    "            st_pb[0].stats.endtime,\n",
    "            len(st_pb[0].data),\n",
    "            prob_n,\n",
    "            prob_p,\n",
    "            prob_s,\n",
    "            n_waves,\n",
    "        )\n",
    "        # Save probabilities to holder numpy arrays (again, this can be accelerated by preallocating space)\n",
    "        PROB_N = np.append(PROB_N, prob_n)\n",
    "        PROB_P = np.append(PROB_P, prob_p)\n",
    "        PROB_S = np.append(PROB_S, prob_s)\n",
    "\n",
    "        ### write probabilities to collections that serve as sliding (read-write buffers?)\n",
    "        # Circular vector probability\n",
    "        buffer_proba.append(prob_p) # append prob_p to right side of p-probabilitiy deque ('deck')\n",
    "        buffer_proba_np = np.array(buffer_proba) # Take the list-like \"collection\", convert to np array, and \n",
    "        buffer_proba_np = np.flip(buffer_proba_np)  # First element is the current proba\n",
    "        breakpoint()\n",
    "        buffer_proba_n.append(prob_n)  # Prob of being N\n",
    "        buffer_proba_of = np.array(buffer_proba_n)\n",
    "        buffer_proba_of = np.flip(buffer_proba_of)  # First element is the current proba\n",
    "\n",
    "        ## TODO: Logic for this trigger can probably be cleaned up with obspy.signal.trigger_onset?\n",
    "        # Logic for trigger\n",
    "\n",
    "        # If the 3-element mean probability surpasses a threshold and there are 3+ data to work with, start triggering process\n",
    "        if np.mean(buffer_proba_np[0:3]) >= thr and len(buffer_proba_np) >= 3:\n",
    "            eq_thr_present = 1\n",
    "\n",
    "        # If trigger is ON, do sanity check if high probability noise classification negates this\n",
    "        if eq_thr_present == 1:\n",
    "            # If there is a high probability of being noise, reject earlier trigger on\n",
    "            if np.mean(buffer_proba_of) >= 0.99:\n",
    "                eq_thr_present = 0\n",
    "\n",
    "        # If this is INDEED a new event, run pick prediction\n",
    "        if eq_thr_past == 0 and eq_thr_present == 1:\n",
    "            print(\"New event!:\")\n",
    "\n",
    "            # ANOTHER FIXED PARAMETER?? - TODO: Move this up to a parameter_control block...?\n",
    "            w_sec_pick = 8\n",
    "            st_pick = st_pb.copy().trim(\n",
    "                st_pb[0].stats.endtime - w_sec_pick, st_pb[0].stats.endtime\n",
    "            )\n",
    "\n",
    "            ## Run P-pick prediction - AGAIN , LOTS OF REPEATED PRE-PROCESSING\n",
    "            PROB_PP = pick_func.PP_pick(\n",
    "                st_pick, mov_time, pb_inst, pzfile, PICK_pb, fmin=1.0, fmax=7.0\n",
    "            )  # SASPe for use pzfile, STEAD for not\n",
    "\n",
    "\n",
    "            # Take peak probability value as P-pick, regardless of probability amplitude....\n",
    "            P_AI_date = (\n",
    "                st_pick[0].stats.starttime + np.argmax(PROB_PP) * mov_time + 4 - Ptrain\n",
    "            )\n",
    "            print(\"P arrival at:\", P_AI_date)  # Must delete\n",
    "\n",
    "            # If saving data for plotting, dump a bunch of stuff to holder arrays.\n",
    "            if flag_plot == 1:\n",
    "                REF = np.append(REF, st_pick[0].stats.starttime - st[0].stats.starttime)\n",
    "                if len(REF) == 1:\n",
    "                    PROB_PP_TENSOR = np.array([PROB_PP])\n",
    "                    P_AI_DATE_VECTOR = [str(P_AI_date)]\n",
    "                else:\n",
    "                    PROB_PP_TENSOR = np.vstack((PROB_PP_TENSOR, PROB_PP))\n",
    "                    P_AI_DATE_VECTOR.append(str(P_AI_date))\n",
    "\n",
    "            # Switch on magnitude calculation \n",
    "            # Calculate magnitude\n",
    "            flag_magnitude = 1  # Change 1 to calculate source parameters\n",
    "\n",
    "        # TODO: Change a lot of these flag_* things to type BOOL...\n",
    "        ## THIS IS THE GUTS OF WHAT WE WANT TO PLAY WITH\n",
    "        if flag_magnitude == 1:\n",
    "            print(\"MAG_time:\", st_pb[0].stats.endtime - P_AI_date)\n",
    "            # Wait until trace has minimum 3.0 sec\n",
    "            if st_pb[0].stats.endtime - P_AI_date >= 3.0:\n",
    "                st_pb_reg = st_process.copy().trim(P_AI_date - 7, P_AI_date + 3 + c_mag)\n",
    "\n",
    "                # FV for REG and BAZ\n",
    "                FV45 = pb_utils.st_FV(\n",
    "                    st_pb_reg, pb_inst, pzfile=pzfile, fmin=1.0, fmax=45.0\n",
    "                )\n",
    "                FV45 = np.real([FV45])\n",
    "\n",
    "                # Regression for MAG, DIS and DEP\n",
    "                MAG_pb_predict = MAG_pb[c_mag].predict(FV45)[0]\n",
    "                DIS_pb_predict = DIS_pb[c_mag].predict(FV45)[0]\n",
    "                DEP_pb_predict = DEP_pb[c_mag].predict(FV45)[0]\n",
    "                if c_mag <= n_models_baz - 1:\n",
    "                    BAZ_pb_cos_predict = BAZ_cos_pb[c_mag].predict(FV45)[0]\n",
    "                    BAZ_pb_sin_predict = BAZ_sin_pb[c_mag].predict(FV45)[0]\n",
    "\n",
    "                    # Cos, Sin, to BAZ angle\n",
    "                    BAZ_pb_predict = (\n",
    "                        np.arctan2(BAZ_pb_sin_predict, BAZ_pb_cos_predict) * 180 / np.pi\n",
    "                    )\n",
    "\n",
    "                    # Get eq.lat, eq.lon\n",
    "                    eq_lat, eq_lon = pb_utils.pb_getpoint(\n",
    "                        sta_lat, sta_lon, DIS_pb_predict, BAZ_pb_predict\n",
    "                    )\n",
    "                    eq_lat = round(eq_lat, 6)\n",
    "                    eq_lon = round(eq_lon, 6)\n",
    "\n",
    "                print(\n",
    "                    \"-->\",\n",
    "                    P_AI_date,\n",
    "                    st_pb_reg[0].stats.starttime,\n",
    "                    st_pb_reg[0].stats.endtime,\n",
    "                    st_pb_reg[0].stats.endtime - st_pb_reg[0].stats.starttime,\n",
    "                    MAG_pb_predict,\n",
    "                    eq_lat,\n",
    "                    eq_lon,\n",
    "                    DIS_pb_predict,\n",
    "                    DEP_pb_predict,\n",
    "                )\n",
    "\n",
    "                if flag_trackmag == True and flag_writefile == 1:\n",
    "                    f = open(folder_out + \"/\" + station + \"_\" + eq_name + \".csv\", \"a\")\n",
    "                    f.write(\n",
    "                        str(P_AI_date)\n",
    "                        + \",\"\n",
    "                        + str(st_pb_reg[0].stats.starttime)\n",
    "                        + \",\"\n",
    "                        + str(st_pb_reg[0].stats.endtime)\n",
    "                        + \",\"\n",
    "                        + str(\n",
    "                            st_pb_reg[0].stats.endtime\n",
    "                            - st_pb_reg[0].stats.starttime\n",
    "                            - 7\n",
    "                        )\n",
    "                        + \",\"\n",
    "                        + str(MAG_pb_predict)\n",
    "                        + \",\"\n",
    "                        + str(eq_lat)\n",
    "                        + \",\"\n",
    "                        + str(eq_lon)\n",
    "                        + \",\"\n",
    "                        + str(DIS_pb_predict)\n",
    "                        + \",\"\n",
    "                        + str(DEP_pb_predict)\n",
    "                        + \",\"\n",
    "                        + str(BAZ_pb_predict)\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "                    f.close()\n",
    "\n",
    "                # Save first magnitude estimation just for plot\n",
    "                if c_mag == 0:\n",
    "                    MAG_3s = MAG_pb_predict\n",
    "\n",
    "                c_mag = c_mag + 1\n",
    "\n",
    "        # computing magnitude c_mag times\n",
    "        if c_mag == n_models:\n",
    "            flag_magnitude = 0\n",
    "            c_mag = 0\n",
    "\n",
    "        # Save past values\n",
    "        eq_thr_past = eq_thr_present"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E3WS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
